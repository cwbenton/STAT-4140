{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bdd1a04",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "XGBoost, in its simplest form, is a type of flow chart. We take our entire dataset and make a decision tree from it. We get the results and use that to make another decision tree. We keep doing this until we get a training accuracy that is sufficient for us. The result is what we get out of our last decision tree.\\\n",
    "In the case of classification, the mis-classified records would be added to the loss function. Specifically, it is the Cross-Entropy Loss Function. If we had a regression loss function, it would most likely be a Mean Square Error Loss Function. For regression problems, we do not calculate accuracy, we use the CELF in classification and the MSLF in regression (there are others, but these are the most common.)\\\n",
    "How do we know where to stop in XGBoost? That is where we use gradients. If we graph the loss function on the y-axis and the predicted outcomes on the x-axis, we can see a minimum value for the loss function, that would be ideal since we want to minimize loss. The change in the loss function over the change in the predicted outcome is the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f34d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.47390177839101205\n"
     ]
    }
   ],
   "source": [
    "#Regression\n",
    "from sklearn.datasets import fetch_california_housing, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Grab the dataset.\n",
    "diabetes = fetch_california_housing()\n",
    "\n",
    "# Separate the data\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "# Since this is a regression problem, a squared error objective makes sense here.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=42)  \n",
    "\n",
    "# Fit the model to our training data.\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# For regression, we show the MSE, not the accuracy.\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Root Mean Squared Error:\", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d340263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[43  0]\n",
      " [ 0 71]]\n"
     ]
    }
   ],
   "source": [
    "#Classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Grab the data\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Separate the data\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Run the model\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Predict using the model with validation set.\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# This is a better metric than accuracy.\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ce049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
